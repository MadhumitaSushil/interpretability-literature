# Another collection of interpretability-related papers

## Overviews 

Molnar. [Interpretable machine learning. A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/). 2019.

## Perspectives 

Zachary C. Lipton. [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490). In *WHI 2016*.

Goodman and Flaxman. [European Union regulations on algorithmic decision-making and a "right to explanation"]. In *WHI 2016*.

## Evaluation of post-hoc explanatory methods

Feng et al. [Pathologies of Neural Models Make Interpretations Difficult](https://www.aclweb.org/anthology/D18-1407/). In *EMNLP 2018*.

 * Input reduction iteratively removes the least important word from the input.    
 * The remaining words appear nonsensical to humans and are not the ones determined as important by interpretation method. 

Poerner et al. [Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement](https://www.aclweb.org/anthology/P18-1032/). In *ACL 2018*.

  * Important characterization of explanation: 
  
  > A good explanation method should not reflect what humans attend to, but what task methods attend to.

  * Interpretability differs between *small contexts* NLP tasks and *large context* tasks. 
  
## Self-explanatory models


### Textual explanations generation 

Kim et al. [Textual Explanations for Self-Driving Vehicles](https://arxiv.org/abs/1807.11546). In *ECCV 2018*.


## Lectures 

[Interpretability and Explainability in Machine Learning at Harvard University](https://interpretable-ml-class.github.io/)



